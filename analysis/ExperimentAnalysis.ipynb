{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a99037",
   "metadata": {},
   "source": [
    "# Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eb471",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GENERAL\n",
    "# Experiment directory path\n",
    "EXPERIMENT_DIRPATH = \"sample\"\n",
    "\n",
    "########## EXECUTION LOGS\n",
    "# Number of data points to be skipped in the beginning\n",
    "SKIP_N = 1000\n",
    "# Unit prefixes (options: \"milli\", \"micro\", \"nano\")\n",
    "TIMESTAMP_UNIT_PREFIX = \"milli\"\n",
    "LATENCY_UNIT_PREFIX = \"micro\"\n",
    "# Function to aggregate latency measurements (options: \"mean\", \"min\", \"max\")\n",
    "LATENCY_AGGREGATE_FUNC = \"mean\"\n",
    "# Number of latency histogram bins\n",
    "N_LATENCY_HIST_BINS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022da8a",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "UNIT_PREFIX_FACTOR = {\"nano\": 10**9, \"micro\": 10**6, \"milli\": 10**3}\n",
    "UNIT_PREFIX_SYMBOL = {\"nano\": \"n\", \"micro\": \"u\", \"milli\": \"m\"}\n",
    "\n",
    "# Utilities\n",
    "def get_benchmark_logs_df(benchmark):\n",
    "  df = pd.read_csv(os.path.join(EXPERIMENT_DIRPATH, benchmark + \".csv\"))\n",
    "  return df.drop(index=df.index[:SKIP_N])\n",
    "\n",
    "def list_benchmarks():\n",
    "  return [filename.split(\".\")[0] for filename in os.listdir(EXPERIMENT_DIRPATH) if os.path.isfile(os.path.join(EXPERIMENT_DIRPATH, filename)) and filename.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159875d0",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45129a99",
   "metadata": {},
   "source": [
    "### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = list_benchmarks()\n",
    "fig = plt.figure(figsize=(12 * len(benchmarks), 8 * len(benchmarks)))\n",
    "for (i, benchmark) in enumerate(benchmarks):\n",
    "  df = get_benchmark_logs_df(benchmark)\n",
    "  df[\"window\"] = df.apply(lambda r: int(r[\"timestamp\"] * UNIT_PREFIX_FACTOR.get(TIMESTAMP_UNIT_PREFIX, 1)), axis=1)\n",
    "  df = df.groupby([\"window\"])[\"window\"].count()\n",
    "  df = df.reindex(range(0, int(df.index.max()) + 1), fill_value=0)\n",
    "  ax = fig.add_subplot(len(benchmarks), 1, i + 1)\n",
    "  ax.set_xlim((0, df.index.max()))\n",
    "  ax.grid(alpha=0.75)\n",
    "  df.plot(ax=ax,\n",
    "          title=\"Throughput: %s\" % benchmark,\n",
    "          xlabel=\"Time (%ss)\" % UNIT_PREFIX_SYMBOL.get(TIMESTAMP_UNIT_PREFIX, \"\"),\n",
    "          ylabel=\"Count (Executions)\",\n",
    "          color=\"blue\",\n",
    "          grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af71c46",
   "metadata": {},
   "source": [
    "### Instantaneous Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75722c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = list_benchmarks()\n",
    "fig = plt.figure(figsize=(12 * len(benchmarks), 8 * len(benchmarks)))\n",
    "for (i, benchmark) in enumerate(benchmarks):\n",
    "  df = get_benchmark_logs_df(benchmark)\n",
    "  df[\"window\"] = df.apply(lambda r: int(r[\"timestamp\"] * UNIT_PREFIX_FACTOR.get(TIMESTAMP_UNIT_PREFIX, 1)), axis=1)\n",
    "  df[\"latency\"] = df.apply(lambda r: int(r[\"latency\"] * UNIT_PREFIX_FACTOR.get(LATENCY_UNIT_PREFIX, 1)), axis=1)\n",
    "  df = df.groupby([\"window\"])[\"latency\"].agg(LATENCY_AGGREGATE_FUNC)\n",
    "  ax = fig.add_subplot(len(benchmarks), 1, i + 1)\n",
    "  ax.set_xlim((0, df.index.max()))\n",
    "  ax.grid(alpha=0.75)\n",
    "  df.interpolate(method=\"linear\").plot(ax=ax,\n",
    "                                       title=\"Latency: %s\" % benchmark,\n",
    "                                       xlabel=\"Time (%ss)\" % UNIT_PREFIX_SYMBOL.get(TIMESTAMP_UNIT_PREFIX, \"\"),\n",
    "                                       ylabel=\"Latency (%ss)\" % UNIT_PREFIX_SYMBOL.get(LATENCY_UNIT_PREFIX, \"\"),\n",
    "                                       color=\"purple\",\n",
    "                                       grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522a91f",
   "metadata": {},
   "source": [
    "### Latency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = list_benchmarks()\n",
    "fig = plt.figure(figsize=(12 * len(benchmarks), 8 * len(benchmarks)))\n",
    "for (i, benchmark) in enumerate(benchmarks):\n",
    "  df = get_benchmark_logs_df(benchmark)\n",
    "  df[\"latency\"] = df.apply(lambda r: int(r[\"latency\"] * UNIT_PREFIX_FACTOR.get(LATENCY_UNIT_PREFIX, 1)), axis=1)\n",
    "  ax = fig.add_subplot(len(benchmarks), 1, i + 1)\n",
    "  ax.grid(alpha=0.75)\n",
    "  ax.set_yscale(\"log\")\n",
    "  ax.set_xlim((0, df[\"latency\"].max()))\n",
    "  ax.set_xlabel(\"Latency (%ss)\" % UNIT_PREFIX_SYMBOL.get(LATENCY_UNIT_PREFIX, \"\"))\n",
    "  df[\"latency\"].plot(ax=ax,\n",
    "                     bins=N_LATENCY_HIST_BINS,\n",
    "                     kind=\"hist\",\n",
    "                     title=\"Latency Distribution: %s benchmark\" % benchmark,\n",
    "                     grid=True,\n",
    "                     color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7baf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = list_benchmarks()\n",
    "fig = plt.figure(figsize=(12 * len(benchmarks), 8 * len(benchmarks)))\n",
    "for (i, benchmark) in enumerate(benchmarks):\n",
    "  df = get_benchmark_logs_df(benchmark)\n",
    "  df[\"latency\"] = df.apply(lambda r: int(r[\"latency\"] * UNIT_PREFIX_FACTOR.get(LATENCY_UNIT_PREFIX, 1)), axis=1)\n",
    "  ax = fig.add_subplot(len(benchmarks), 1, i + 1)\n",
    "  ax.grid(alpha=0.75)\n",
    "  ax.set_xlim((0, df[\"latency\"].max()))\n",
    "  ax.set_ylim((0, 1))\n",
    "  ax.set_xlabel(\"Latency (%ss)\" % UNIT_PREFIX_SYMBOL.get(LATENCY_UNIT_PREFIX, \"\"))\n",
    "  df[\"latency\"].plot(ax=ax,\n",
    "                     bins=N_LATENCY_HIST_BINS,\n",
    "                     kind=\"hist\",\n",
    "                     cumulative=True,\n",
    "                     density=True,\n",
    "                     histtype=\"step\",\n",
    "                     title=\"Latency Distribution: %s benchmark\" % benchmark,\n",
    "                     grid=True,\n",
    "                     color=\"green\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
